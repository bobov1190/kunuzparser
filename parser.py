"""
KunUz Parser - –£–¥–æ–±–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π
–ü—Ä–æ—Å—Ç–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: kunuzparser() –∏–ª–∏ kunuzparser('health', limit=10)
"""

import re
from typing import List, Dict, Optional, Union
from datetime import datetime
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright
from pathlib import Path
import json

from config import CATEGORIES, BASE_URL, OUTPUT_DIR


FOOTER_RE = re.compile(
    r'"?KUN\.UZ"? saytida e ºlon qilingan materiallardan.*$',
    re.DOTALL | re.IGNORECASE
)


class KunUzParser:
    """–ü–∞—Ä—Å–µ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π —Å Kun.uz"""
    
    def __init__(self):
        self.output_dir = Path(OUTPUT_DIR)
        self.output_dir.mkdir(exist_ok=True)

    def clean_content(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –æ—Ç —Ñ—É—Ç–µ—Ä–æ–≤ –∏ –ª–∏—à–Ω–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
        text = re.sub(FOOTER_RE, "", text or "")
        text = re.sub(
            r'\b(Foto|–§–æ—Ç–æ|Surat|Rasm)\s*:\s*[^.]+\.*',
            '',
            text,
            flags=re.IGNORECASE
        )
        return re.sub(r"\s+", " ", text).strip()

    def extract_date(self, soup: BeautifulSoup) -> Optional[datetime]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏"""
        meta = soup.select_one('meta[property="article:published_time"]')
        if meta and meta.get("content"):
            try:
                return datetime.fromisoformat(meta["content"].replace("Z", ""))
            except Exception:
                pass

        m = re.search(r'(\d{2})\.(\d{2})\.(\d{4})', soup.get_text())
        if m:
            return datetime(int(m.group(3)), int(m.group(2)), int(m.group(1)))
        return None

    def date_allowed(
        self, 
        date: Optional[datetime], 
        from_date: Optional[datetime], 
        to_date: Optional[datetime]
    ) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ø–∞–¥–∞–Ω–∏—è –¥–∞—Ç—ã –≤ –¥–∏–∞–ø–∞–∑–æ–Ω"""
        if not date:
            return True
        if from_date and date < from_date:
            return False
        if to_date and date > to_date:
            return False
        return True

    def scroll_until_button(self, page) -> bool:
        """–ü—Ä–æ–∫—Ä—É—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–æ –∫–Ω–æ–ø–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏"""
        for _ in range(6):
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            page.wait_for_timeout(1200)
            if page.query_selector("div.point-view__footer button"):
                return True
        return False

    def fetch_list(self, page, url: str, limit: int) -> List[str]:
        """–°–±–æ—Ä —Å–ø–∏—Å–∫–∞ URL –Ω–æ–≤–æ—Å—Ç–µ–π"""
        page.goto(url, wait_until="networkidle")
        collected = set()

        while len(collected) < limit:
            soup = BeautifulSoup(page.content(), "lxml")

            for a in soup.select("a.news-page__item[href]"):
                href = a.get("href")
                if href and re.search(r"/news/\d{4}/\d{2}/\d{2}/", href):
                    collected.add(BASE_URL + href)
                    if len(collected) >= limit:
                        break

            if len(collected) >= limit:
                break

            if not self.scroll_until_button(page):
                break

            page.click("div.point-view__footer button")
            page.wait_for_timeout(1800)

        return list(collected)[:limit]

    def fetch_detail(
        self, 
        page, 
        url: str, 
        category: str,
        from_date: Optional[datetime],
        to_date: Optional[datetime]
    ) -> Optional[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–æ–≤–æ—Å—Ç–∏"""
        try:
            page.goto(url, wait_until="networkidle", timeout=30000)
        except Exception:
            return None
            
        soup = BeautifulSoup(page.content(), "lxml")

        h1 = soup.find("h1")
        if not h1:
            return None

        published_at = self.extract_date(soup)
        if not self.date_allowed(published_at, from_date, to_date):
            return None

        block = (
            soup.select_one("div.single-content")
            or soup.select_one("div.news-inner__content")
        )
        if not block:
            return None

        for tag in block.select("script, style, figure, iframe, .share, .ads"):
            tag.decompose()

        text = " ".join(
            p.get_text(" ", strip=True)
            for p in block.find_all("p")
            if "cookies" not in p.get_text(strip=True).lower()
        )

        content = self.clean_content(text)
        if len(content) < 200:
            return None

        img = soup.select_one('meta[property="og:image"]')

        return {
            "title": h1.get_text(strip=True),
            "content": content,
            "image_url": img["content"] if img else None,
            "published_at": published_at.isoformat() if published_at else None,
            "source_url": url,
            "source": "kunuz",
            "category": category,
            "language": "uz"
        }

    def parse_category(
        self,
        page,
        key: str,
        limit: int,
        from_date: Optional[datetime],
        to_date: Optional[datetime],
        verbose: bool = True
    ) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        cfg = CATEGORIES.get(key)
        if not cfg:
            if verbose:
                print(f"‚ùå –ö–∞—Ç–µ–≥–æ—Ä–∏—è '{key}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return []

        if verbose:
            print(f"üöÄ –ü–∞—Ä—Å–∏–Ω–≥: {key} (–ª–∏–º–∏—Ç: {limit})")
        
        urls = self.fetch_list(page, cfg["url"], limit)
        if verbose:
            print(f"   –ù–∞–π–¥–µ–Ω–æ URL: {len(urls)}")

        results = []
        for i, url in enumerate(urls, 1):
            item = self.fetch_detail(page, url, cfg["category_name"], from_date, to_date)
            if item:
                results.append(item)
                if verbose:
                    print(f"   ‚úì [{i}/{len(urls)}] {item['title'][:50]}...")
            else:
                if verbose:
                    print(f"   ‚úó [{i}/{len(urls)}] –ü—Ä–æ–ø—É—â–µ–Ω–æ")

        return results

    def parse(
        self,
        category: Union[str, List[str]] = 'everything',
        limit: int = 20,
        from_date: Optional[str] = None,
        to_date: Optional[str] = None,
        save: bool = True,
        verbose: bool = True
    ) -> List[Dict]:
        """
        –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        
        Args:
            category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –∏–ª–∏ —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π ('health', 'world', 'everything')
            limit: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π (default: 20)
            from_date: –î–∞—Ç–∞ –Ω–∞—á–∞–ª–∞ 'YYYY-MM-DD'
            to_date: –î–∞—Ç–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è 'YYYY-MM-DD'
            save: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ –≤ JSON —Ñ–∞–π–ª
            verbose: –í—ã–≤–æ–¥–∏—Ç—å –ª–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π (list of dict)
        """
        
        # –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç
        fd = datetime.strptime(from_date, "%Y-%m-%d") if from_date else None
        td = datetime.strptime(to_date, "%Y-%m-%d") if to_date else None

        all_results = []

        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()

            # Everything - —Å–æ–±–∏—Ä–∞–µ–º —Å–æ –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            if category == 'everything':
                if verbose:
                    print(f"\nüåü –†–ï–ñ–ò–ú: EVERYTHING (–≤—Å–µ–≥–æ {limit} –Ω–æ–≤–æ—Å—Ç–µ–π)")
                
                categories_list = list(CATEGORIES.keys())
                per_category = max(1, limit // len(categories_list))
                remainder = limit % len(categories_list)
                
                for i, key in enumerate(categories_list):
                    cat_limit = per_category + (1 if i < remainder else 0)
                    results = self.parse_category(page, key, cat_limit, fd, td, verbose)
                    all_results.extend(results)
                    
                    if len(all_results) >= limit:
                        all_results = all_results[:limit]
                        break
            
            # –û–¥–Ω–∞ –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            else:
                cats = [category] if isinstance(category, str) else category
                
                for key in cats:
                    results = self.parse_category(page, key, limit, fd, td, verbose)
                    all_results.extend(results)

            page.close()
            browser.close()

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        if save and all_results:
            filename = f"kunuz_{category if isinstance(category, str) else 'multiple'}.json"
            path = self.output_dir / filename
            
            with open(path, "w", encoding="utf-8") as f:
                json.dump(all_results, f, ensure_ascii=False, indent=2)
            
            if verbose:
                print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {len(all_results)} –Ω–æ–≤–æ—Å—Ç–µ–π ‚Üí {path}")

        if verbose:
            print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ! –í—Å–µ–≥–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(all_results)}")
        
        return all_results


def kunuzparser(
    category: Union[str, List[str]] = 'everything',
    limit: int = 20,
    from_date: Optional[str] = None,
    to_date: Optional[str] = None,
    save: bool = True,
    verbose: bool = True
) -> List[Dict]:
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π —Å Kun.uz
    
    –ü—Ä–∏–º–µ—Ä—ã:
        # –í—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (20 –Ω–æ–≤–æ—Å—Ç–µ–π)
        kunuzparser()
        
        # –ó–¥–æ—Ä–æ–≤—å–µ (10 –Ω–æ–≤–æ—Å—Ç–µ–π)
        kunuzparser('health', limit=10)
        
        # –ù–µ—Å–∫–æ–ª—å–∫–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        kunuzparser(['health', 'sport'], limit=30)
        
        # –° —Ñ–∏–ª—å—Ç—Ä–æ–º –ø–æ –¥–∞—Ç–µ
        kunuzparser('world', from_date='2026-01-25')
    
    –ö–∞—Ç–µ–≥–æ—Ä–∏–∏:
        - health (–∑–¥–æ—Ä–æ–≤—å–µ)
        - world (–º–∏—Ä–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏)
        - economy (—ç–∫–æ–Ω–æ–º–∏–∫–∞)
        - sport (—Å–ø–æ—Ä—Ç)
        - technology (—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏)
        - education (–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ)
        - useful (—Ç—É—Ä–∏–∑–º/—Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏—è)
        - everything (–≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏)
    
    Args:
        category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –∏–ª–∏ —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        limit: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π
        from_date: –î–∞—Ç–∞ –Ω–∞—á–∞–ª–∞ 'YYYY-MM-DD'
        to_date: –î–∞—Ç–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è 'YYYY-MM-DD'
        save: –°–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON
        verbose: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å
        
    Returns:
        –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π (list of dict)
    """
    parser = KunUzParser()
    return parser.parse(category, limit, from_date, to_date, save, verbose)
